# ğŸš€ End-to-End ELT Pipeline | Snowflake + dbt + Airflow (Cosmos)

## ğŸ“Œ Project Overview

This project implements a production-style ELT data pipeline using a modern data stack.

The pipeline processes the **Brazilian E-Commerce Public Dataset (Olist)** and transforms raw data into analytics-ready models using a layered transformation architecture.

Orchestration is handled by Apache Airflow with dynamic DAG generation powered by Cosmos, running via Astro.

---

## ğŸ— Architecture Overview

```
Raw CSV Data (Olist - Kaggle)
â†“
Snowflake (RAW Layer)
â†“
dbt (Staging Layer)
â†“
dbt (Intermediate Layer)
â†“   
dbt (Marts Layer - Star Schema)
â†“
Airflow + Cosmos (Orchestration)
```
---

## ğŸ”¹ Tech Stack

- **Snowflake** â€“ Cloud Data Warehouse
- **dbt** â€“ Transformation & Data Modeling
- **Apache Airflow** â€“ Workflow Orchestration
- **Astronomer Cosmos** â€“ Dynamic dbt DAG generation
- **Astro CLI** â€“ Local Airflow runtime environment

---

## ğŸ§± Data Modeling Strategy

### 1ï¸âƒ£ RAW Layer (Snowflake)
- Data loaded as-is
- No transformation
- Source of truth

### 2ï¸âƒ£ Staging Layer (dbt)
- Column renaming (snake_case)
- Type casting (timestamps, numerics)
- Status normalization
- Deduplication
- Data quality tests

### 3ï¸âƒ£ Intermediate Layer (dbt)
- Business logic transformations
- Revenue calculations
- Table joins
- Grain enforcement

### 4ï¸âƒ£ Marts Layer (Star Schema)
- `dim_customers`
- `dim_products`
- `fct_orders`
- `fct_order_items`

---

## ğŸ” Incremental Models

Fact tables are implemented as incremental models:

- Efficient processing of new data
- Scalable for large datasets

---

## ğŸ•° Slowly Changing Dimensions (SCD Type 2)

Implemented using dbt snapshots:

- Tracks customer attribute changes
- Maintains historical state
- Enables time-based analysis

---

## ğŸ§ª Data Quality & Testing

Implemented dbt tests:

- `not_null`
- `unique`
- `accepted_values`
- `relationships`
- Composite primary key validation

Ensures:
- Referential integrity
- No duplicate primary keys
- Proper grain enforcement

---

## âš™ Orchestration with Airflow + Cosmos

Airflow dynamically generates DAGs from the dbt project using Cosmos.

Benefits:

- Each dbt model becomes an Airflow task
- Automatic dependency resolution
- Model-level retry capability
- Clear observability in Airflow UI

Execution Flow:

```
 Staging â†’ Intermediate â†’ Marts â†’ Snapshots â†’ Tests
```



---

## ğŸš€ How to Run Locally

- **Note** : Make sure you have started the docker engine started before running the code.
    
1. Start Astro:

```bash
astro dev start
 ```
2. Open Airflow UI:
```
http://localhost:8080
```

3. Configure Snowflake connection in Airflow

4. Trigger the dbt_olist_cosmos DAG

Output:

![image](my_dag-graph.png)
